{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thiya\\Anaconda2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Thiya\\Anaconda2\\lib\\site-packages\\tensorflow\\python\\training\\queue_runner_impl.py:391: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Restoring parameters from C:/Users/Thiya/Desktop/Wayne/Sofwerx/Project/photos/face2face-model_7/model-1600\n",
      "Model is exported to C:/Users/Thiya/Desktop/Wayne/Sofwerx/Project/photos/face2face-model_7/model-1600\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "CROP_SIZE = 256  # scale_size = CROP_SIZE\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    with tf.name_scope('preprocess'):\n",
    "        # [0, 1] => [-1, 1]\n",
    "        return image * 2 - 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def deprocess(image):\n",
    "    with tf.name_scope('deprocess'):\n",
    "        # [-1, 1] => [0, 1]\n",
    "        return (image + 1) / 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv(batch_input, out_channels, stride):\n",
    "    with tf.variable_scope('conv'):\n",
    "        in_channels = batch_input.get_shape()[3]\n",
    "        filter = tf.get_variable('filter', [4, 4, in_channels, out_channels], dtype=tf.float32,\n",
    "                                 initializer=tf.random_normal_initializer(0, 0.02))\n",
    "        # [batch, in_height, in_width, in_channels], [filter_width, filter_height, in_channels, out_channels]\n",
    "        #     => [batch, out_height, out_width, out_channels]\n",
    "        padded_input = tf.pad(batch_input, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')\n",
    "        conv = tf.nn.conv2d(padded_input, filter, [1, stride, stride, 1], padding='VALID')\n",
    "        return conv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lrelu(x, a):\n",
    "    with tf.name_scope('lrelu'):\n",
    "        # adding these together creates the leak part and linear part\n",
    "        # then cancels them out by subtracting/adding an absolute value term\n",
    "        # leak: a*x/2 - a*abs(x)/2\n",
    "        # linear: x/2 + abs(x)/2\n",
    "        # this block looks like it has 2 inputs on the graph unless we do this\n",
    "        x = tf.identity(x)\n",
    "        return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batchnorm(input):\n",
    "    with tf.variable_scope('batchnorm'):\n",
    "        # this block looks like it has 3 inputs on the graph unless we do this\n",
    "        input = tf.identity(input)\n",
    "        channels = input.get_shape()[3]\n",
    "        offset = tf.get_variable('offset', [channels], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "        scale = tf.get_variable('scale', [channels], dtype=tf.float32,\n",
    "                                initializer=tf.random_normal_initializer(1.0, 0.02))\n",
    "        mean, variance = tf.nn.moments(input, axes=[0, 1, 2], keep_dims=False)\n",
    "        variance_epsilon = 1e-5\n",
    "        normalized = tf.nn.batch_normalization(input, mean, variance, offset, scale, variance_epsilon=variance_epsilon)\n",
    "        return normalized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def deconv(batch_input, out_channels):\n",
    "    with tf.variable_scope('deconv'):\n",
    "        batch, in_height, in_width, in_channels = [int(d) for d in batch_input.get_shape()]\n",
    "        filter = tf.get_variable('filter', [4, 4, out_channels, in_channels], dtype=tf.float32,\n",
    "                                 initializer=tf.random_normal_initializer(0, 0.02))\n",
    "        # [batch, in_height, in_width, in_channels], [filter_width, filter_height, out_channels, in_channels]\n",
    "        #     => [batch, out_height, out_width, out_channels]\n",
    "        conv = tf.nn.conv2d_transpose(batch_input, filter, [batch, in_height * 2, in_width * 2, out_channels],\n",
    "                                      [1, 2, 2, 1], padding='SAME')\n",
    "        return conv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_image(x):\n",
    "    with tf.name_scope('load_images'):\n",
    "        raw_input = tf.image.convert_image_dtype(x, dtype=tf.float32)\n",
    "        raw_input.set_shape([None, None, 3])\n",
    "        # break apart image pair and move to range [-1, 1]\n",
    "        width = tf.shape(raw_input)[1]  # [height, width, channels]\n",
    "        a_images = preprocess(raw_input[:, :width // 2, :])\n",
    "        b_images = preprocess(raw_input[:, width // 2:, :])\n",
    "    inputs, targets = [a_images, b_images]\n",
    "\n",
    "\n",
    "\n",
    "    # synchronize seed for image operations so that we do the same operations to both\n",
    "\n",
    "    # input and output images\n",
    "\n",
    "    def transform(image):\n",
    "        r = image\n",
    "        # area produces a nice downscaling, but does nearest neighbor for upscaling\n",
    "        # assume we're going to be doing downscaling here\n",
    "        r = tf.image.resize_images(r, [CROP_SIZE, CROP_SIZE], method=tf.image.ResizeMethod.AREA)\n",
    "        return r\n",
    "\n",
    "\n",
    "\n",
    "    with tf.name_scope('input_images'):\n",
    "        input_images = tf.expand_dims(transform(inputs), 0)\n",
    "\n",
    "\n",
    "    with tf.name_scope('target_images'):\n",
    "        target_images = tf.expand_dims(transform(targets), 0)\n",
    "\n",
    "    return input_images, target_images\n",
    "\n",
    "\n",
    "\n",
    "    # Tensor('batch:1', shape=(1, 256, 256, 3), dtype=float32) -> 1 batch size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_generator(generator_inputs, generator_outputs_channels):\n",
    "    layers = []\n",
    "    # encoder_1: [batch, 256, 256, in_channels] => [batch, 128, 128, ngf]\n",
    "    with tf.variable_scope('encoder_1'):\n",
    "        output = conv(generator_inputs, ngf, stride=2)\n",
    "        layers.append(output)\n",
    "\n",
    "\n",
    "\n",
    "    layer_specs = [\n",
    "\n",
    "        ngf * 2,  # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]\n",
    "\n",
    "        ngf * 4,  # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]\n",
    "\n",
    "        ngf * 8,  # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]\n",
    "\n",
    "        ngf * 8,  # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]\n",
    "\n",
    "        ngf * 8,  # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]\n",
    "\n",
    "        ngf * 8,  # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]\n",
    "\n",
    "        ngf * 8,  # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]\n",
    "\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    for out_channels in layer_specs:\n",
    "\n",
    "        with tf.variable_scope('encoder_%d' % (len(layers) + 1)):\n",
    "\n",
    "            rectified = lrelu(layers[-1], 0.2)\n",
    "\n",
    "            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]\n",
    "\n",
    "            convolved = conv(rectified, out_channels, stride=2)\n",
    "\n",
    "            output = batchnorm(convolved)\n",
    "\n",
    "            layers.append(output)\n",
    "\n",
    "\n",
    "\n",
    "    layer_specs = [\n",
    "\n",
    "        (ngf * 8, 0.5),  # decoder_8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]\n",
    "\n",
    "        (ngf * 8, 0.5),  # decoder_7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]\n",
    "\n",
    "        (ngf * 8, 0.5),  # decoder_6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]\n",
    "\n",
    "        (ngf * 8, 0.0),  # decoder_5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]\n",
    "\n",
    "        (ngf * 4, 0.0),  # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]\n",
    "\n",
    "        (ngf * 2, 0.0),  # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]\n",
    "\n",
    "        (ngf, 0.0),  # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]\n",
    "\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    num_encoder_layers = len(layers)\n",
    "\n",
    "    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):\n",
    "\n",
    "        skip_layer = num_encoder_layers - decoder_layer - 1\n",
    "\n",
    "        with tf.variable_scope('decoder_%d' % (skip_layer + 1)):\n",
    "\n",
    "            if decoder_layer == 0:\n",
    "\n",
    "                # first decoder layer doesn't have skip connections\n",
    "\n",
    "                # since it is directly connected to the skip_layer\n",
    "\n",
    "                input = layers[-1]\n",
    "\n",
    "            else:\n",
    "\n",
    "                input = tf.concat([layers[-1], layers[skip_layer]], axis=3)\n",
    "\n",
    "\n",
    "\n",
    "            rectified = tf.nn.relu(input)\n",
    "\n",
    "            # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]\n",
    "\n",
    "            output = deconv(rectified, out_channels)\n",
    "\n",
    "            output = batchnorm(output)\n",
    "\n",
    "\n",
    "\n",
    "            if dropout > 0.0:\n",
    "\n",
    "                output = tf.nn.dropout(output, keep_prob=1 - dropout)\n",
    "\n",
    "\n",
    "\n",
    "            layers.append(output)\n",
    "\n",
    "\n",
    "\n",
    "    # decoder_1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, generator_outputs_channels]\n",
    "\n",
    "    with tf.variable_scope('decoder_1'):\n",
    "\n",
    "        input = tf.concat([layers[-1], layers[0]], axis=3)\n",
    "\n",
    "        rectified = tf.nn.relu(input)\n",
    "\n",
    "        output = deconv(rectified, generator_outputs_channels)\n",
    "\n",
    "        output = tf.tanh(output)\n",
    "\n",
    "        layers.append(output)\n",
    "\n",
    "\n",
    "\n",
    "    return layers[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model(inputs, targets):\n",
    "\n",
    "    with tf.variable_scope('generator') as scope:\n",
    "\n",
    "        out_channels = int(targets.get_shape()[-1])\n",
    "\n",
    "        outputs = create_generator(inputs, out_channels)\n",
    "\n",
    "\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert(image):\n",
    "\n",
    "    return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True, name='output')  # output tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_output(x):\n",
    "\n",
    "    with tf.name_scope('generate_output'):\n",
    "\n",
    "        test_inputs, test_targets = process_image(x)\n",
    "\n",
    "\n",
    "\n",
    "        # inputs and targets are [batch_size, height, width, channels]\n",
    "\n",
    "        model = create_model(test_inputs, test_targets)\n",
    "\n",
    "\n",
    "\n",
    "        # deprocess files\n",
    "\n",
    "        outputs = deprocess(model)\n",
    "\n",
    "\n",
    "\n",
    "        # reverse any processing on images so they can be written to disk or displayed to user\n",
    "\n",
    "        converted_outputs = convert(outputs)\n",
    "\n",
    "    return converted_outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--model-input', dest='input_folder', type=str, help='Model folder to import.')\n",
    "\n",
    "    parser.add_argument('--model-output', dest='output_folder', type=str, help='Model (reduced) folder to export.')\n",
    "\n",
    "    #args = parser.parse_args()\n",
    "    args = parser.parse_known_args()\n",
    "\n",
    "\n",
    "    x = tf.placeholder(tf.uint8, shape=(256, 512, 3), name='image_tensor')  # input tensor\n",
    "\n",
    "    y = generate_output(x)\n",
    "\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Restore original model\n",
    "\n",
    "        \n",
    "        saver = tf.train.import_meta_graph('C:/Users/Thiya/Desktop/Wayne/Sofwerx/Project/photos/face2face-model_7/model-1600.meta', clear_devices=True)\n",
    "        \n",
    "        saver.restore(sess,'C:/Users/Thiya/Desktop/Wayne/Sofwerx/Project/photos/face2face-model_7/model-1600') #checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "        # Export reduced model used for prediction\n",
    "\n",
    "       \n",
    "\n",
    "        saver.save(sess, '{}/reduced_model'.format(\"C:/Users/Thiya/Desktop/Wayne/Sofwerx/Project/photos/face2face-reduced1/\"))\n",
    "\n",
    "        print(\"Model is exported to {}\".format(\"C:/Users/Thiya/Desktop/Wayne/Sofwerx/Project/photos/face2face-model_7/model-1600\"))#checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
